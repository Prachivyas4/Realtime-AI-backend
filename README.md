# Realtime-AI-backend

## Overview

This project implements a **realtime conversational AI backend** using:

- FastAPI (async Python framework)
- WebSockets for bidirectional streaming
- OpenAI LLM for conversation
- Supabase Postgres for persistent session and event storage
- Function/tool calling (e.g., fetching current time)
- Post-session automated summary generation

A minimal frontend is included for testing WebSocket connections.

---

## Setup Instructions

### 1. Clone Repository
git clone <repo-url>
cd realtime-ai-backend

### 2. Create a virtual environment 
python3 -m venv venv
source venv/bin/activate

### 3. Install dependencies
pip install -r requirements.txt

### 4. Configure Environment  Variables
Create a .env file in project root

OPENAI_API_KEY=sk-xxxxxx
SUPABASE_URL=https://xxxxx.supabase.co
SUPABASE_SERVICE_ROLE_KEY=eyJhbGciOi...

### 5. Run the backend
python -m uvicorn app.main:app --reload


##Supabase Database Schema

#Session Table
CREATE TABLE sessions (
    session_id UUID PRIMARY KEY,
    user_id TEXT DEFAULT 'anonymous',
    start_time TIMESTAMP DEFAULT NOW(),
    end_time TIMESTAMP,
    duration_seconds INT,
    summary TEXT
);

#Session Event Table
CREATE TABLE session_events (
    id BIGINT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
    session_id UUID REFERENCES sessions(session_id) ON DELETE CASCADE,
    event_type TEXT,
    role TEXT,
    content TEXT,
    created_at TIMESTAMP DEFAULT NOW()
);


#Running and Testing

Open frontend/index.html in a browser.
Enter a message and click Send.
The AI response streams in real time.
Check Supabase:
sessions table → session metadata and summary
session_events table → all messages logged
Close the browser → backend automatically generates session summary.

#Key Design Choices
Async WebSockets: Ensures low-latency token streaming
In-memory conversation state: Minimal memory usage, supplemented by persistent event logging
Function/tool calling: Allows LLM to perform internal operations dynamically
Post-session summarization: Demonstrates automated analysis of conversation history
Modular code structure: Each component (LLM, DB, WebSocket, tasks) is separate for clarity and maintainability
